{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>3348_7</td>\n",
       "      <td>1</td>\n",
       "      <td>Clint Tollinger arrives in a small town lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19858</th>\n",
       "      <td>963_4</td>\n",
       "      <td>0</td>\n",
       "      <td>'Chances Are' a big mistake to see. You could ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22894</th>\n",
       "      <td>5703_7</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm a fan of Crash and Blade Runner and this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12775</th>\n",
       "      <td>5792_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Another made for TV piece of junk! This is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16044</th>\n",
       "      <td>2857_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I've seen plenty of Sci-Fi Channel Original mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment                                             review\n",
       "16891  3348_7          1  Clint Tollinger arrives in a small town lookin...\n",
       "19858   963_4          0  'Chances Are' a big mistake to see. You could ...\n",
       "22894  5703_7          1  I'm a fan of Crash and Blade Runner and this m...\n",
       "12775  5792_1          0  Another made for TV piece of junk! This is an ...\n",
       "16044  2857_1          0  I've seen plenty of Sci-Fi Channel Original mo..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = pd.read_csv('D:/jupyter/DATA_SCIENRE/Tuan5/test.csv', encoding='latin-1')\n",
    "sentiment.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "sentiment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits = [\n",
    "    'This is amazing!',\n",
    "    'ML is the best, yes it is',\n",
    "    'I am not sure about how this is going to end...'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define an object of CountVectorizer() fit and transfom your twits into a 'bag'\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(twits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'best',\n",
       " 'end',\n",
       " 'going',\n",
       " 'how',\n",
       " 'is',\n",
       " 'it',\n",
       " 'ml',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'yes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find in document of CountVectorizer a function that show us list of feature names\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call toarray() on your 'bag' to see the feature vectors\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'am', 'amazing', 'best', 'end', 'going', 'how', 'is', 'it', 'ml', 'not', 'sure', 'the', 'this', 'to', 'yes']\n",
      "[[0.   0.   0.72 0.   0.   0.   0.   0.43 0.   0.   0.   0.   0.   0.55 0.   0.  ]\n",
      " [0.   0.   0.   0.4  0.   0.   0.   0.47 0.4  0.4  0.   0.   0.4  0.   0.   0.4 ]\n",
      " [0.33 0.33 0.   0.   0.33 0.33 0.33 0.2  0.   0.   0.33 0.33 0.   0.25 0.33 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "# Feed the tf-idf Vectorizer with twits using fit_transform()\n",
    "tfidf_vec = tfidf.fit_transform(twits)\n",
    "\n",
    "# Formatting the number to 2 digits after the decimal point by showing on this notebook\n",
    "np.set_printoptions(precision=2)\n",
    "# To print array in one line\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "print(tfidf.get_feature_names())\n",
    "print(tfidf_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 286666),\n",
       " ('a', 154921),\n",
       " ('and', 152512),\n",
       " ('of', 142817),\n",
       " ('to', 132392),\n",
       " ('is', 103103),\n",
       " ('in', 85484),\n",
       " ('I', 65901),\n",
       " ('that', 64475),\n",
       " ('this', 57131),\n",
       " ('it', 54369),\n",
       " ('/><br', 50870),\n",
       " ('was', 46655),\n",
       " ('as', 42466),\n",
       " ('with', 41670),\n",
       " ('for', 41031),\n",
       " ('but', 33733),\n",
       " ('The', 33713),\n",
       " ('on', 30730),\n",
       " ('movie', 30420)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "for twit in sentiment.review:\n",
    "    for word in twit.split(' '):\n",
    "        vocab[word] += 1\n",
    "\n",
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 65901),\n",
       " ('/><br', 50870),\n",
       " ('The', 33713),\n",
       " ('movie', 30420),\n",
       " ('film', 27385),\n",
       " ('one', 20660),\n",
       " ('like', 18110),\n",
       " ('This', 12272),\n",
       " ('would', 11908),\n",
       " ('good', 11421),\n",
       " ('It', 10943),\n",
       " ('really', 10795),\n",
       " ('even', 10594),\n",
       " ('see', 10145),\n",
       " ('-', 9347),\n",
       " ('get', 8762),\n",
       " ('story', 8518),\n",
       " ('much', 8493),\n",
       " ('time', 7747),\n",
       " ('make', 7472)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "vocab_reduced = Counter()\n",
    "# Go through all of the items of vocab using vocab.items() and pick only words that are not in 'stop_words' \n",
    "# and save them in vocab_reduced\n",
    "for w, c in vocab.items():\n",
    "    if not w in stop_words:\n",
    "        vocab_reduced[w]=c\n",
    "\n",
    "vocab_reduced.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like it  :)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create some random texts for testing the function preprocessor()\n",
    "print(preprocessor('I like it :), |||<><>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there,', 'I', 'am', 'loving', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n",
      "['Hi', 'there,', 'I', 'am', 'love', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Split a text into list of words\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Split a text into list of words and apply stemming technic\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# Testing\n",
    "print(tokenizer('Hi there, I am loving this, like with a lot of love'))\n",
    "print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = sentiment['review']\n",
    "y = sentiment['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preproc...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "\n",
    "# A pipeline is what chains several steps together, once the initial exploration is done. \n",
    "# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n",
    "# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n",
    "# they are estimators. Pipeline chains all these together which can then be applied to training data\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=0))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.885\n",
      "confusion matrix:\n",
      " [[2165  338]\n",
      " [ 237 2260]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.86      0.88      2503\n",
      "          1       0.87      0.91      0.89      2497\n",
      "\n",
      "avg / total       0.89      0.89      0.88      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = clf.predict(X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not feel not bad --> Negative, Positive = [0.98 0.02]\n",
      "This model is \"so good\" :)) --> Negative, Positive = [0.25 0.75]\n",
      "we are who we are --> Negative, Positive = [0.49 0.51]\n",
      "its good to be bad sometimes --> Negative, Positive = [0.77 0.23]\n",
      "what a wonderful failure! (sarcasm :))) --> Negative, Positive = [0.3 0.7]\n",
      "People do not like the bad things --> Negative, Positive = [0.97 0.03]\n",
      "We finally have the test result. You are positive --> Negative, Positive = [0.7 0.3]\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"I do not feel not bad\", # Phuc +1\n",
    "    'This model is \"so good\" :))', # Long -1\n",
    "    'we are who we are', # Nghi 0\n",
    "    'its good to be bad sometimes', # PA +1\n",
    "    'what a wonderful failure! (sarcasm :)))', #Phuc +1\n",
    "    'People do not like the bad things', # Chi 0\n",
    "    'We finally have the test result. You are positive', # Long +1\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "pickle.dump(clf, open('logisticRegression.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not feel not bad --> Negative, Positive = [0.98 0.02]\n",
      "This model is \"so good\" :)) --> Negative, Positive = [0.25 0.75]\n",
      "we are who we are --> Negative, Positive = [0.49 0.51]\n",
      "its good to be bad sometimes --> Negative, Positive = [0.77 0.23]\n",
      "what a wonderful failure! (sarcasm :))) --> Negative, Positive = [0.3 0.7]\n",
      "People do not like the bad things --> Negative, Positive = [0.97 0.03]\n",
      "We finally have the test result. You are positive --> Negative, Positive = [0.7 0.3]\n"
     ]
    }
   ],
   "source": [
    "with open('logisticRegression.pkl', 'rb') as model:\n",
    "    reload_model = pickle.load(model)\n",
    "preds = reload_model.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (20000, 3) (20000, 3)\n",
      "Test set: (5000, 3) (5000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = sentiment[['id', 'sentiment', 'review']].values\n",
    "y = sentiment[['id', 'sentiment', 'review']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)\n",
    "\n",
    "print('Training set:', X_train.shape, y_train.shape)\n",
    "print('Test set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Another attempt by modern Japanese directors to redefine the chambara genre. Successful, and not, in varying degrees.<br /><br />Buddhist monk has a vision that he is to slay a legendary (and very active) demon at the Gojoe bridge in order to attain enlightenment. While not at the forefront, Buddhist thought is at the heart of this movie, much like Kurosawa\\'s \\\\Ran\\\\\". It probably what made the movie the most interesting to me although it\\'s nowheres near \\\\\"Ran\\'s\\\\\" league. <br /><br />Stylish visual direction and excellent photography keep the movie mostly interesting throughout the two hour and eighteen minute length. The lead actors are uniformly excellent. The music is really good. The two weaknesses are the story and the fight scenes. The movie drags in the middle which could have been fixed by some prudent editing. And the fight scenes are mostly filmed in blurry close-ups. This works for most of the film but the finale feels like a cheat. Another recent film like this is Tsui Hark\\'s \\\\\"Seven Swords\\\\\", great film but the promised fight scenes are disappointing. Asano really doesn\\'t move like a sword wielding demon, his acting is great but he would be an extra in a traditional chambara fight scene.<br /><br />Good movie, you\\'ll probably find it interesting just don\\'t expect traditional sword fights.\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e40755c532a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create Logistics Regression model from X and y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Another attempt by modern Japanese directors to redefine the chambara genre. Successful, and not, in varying degrees.<br /><br />Buddhist monk has a vision that he is to slay a legendary (and very active) demon at the Gojoe bridge in order to attain enlightenment. While not at the forefront, Buddhist thought is at the heart of this movie, much like Kurosawa\\'s \\\\Ran\\\\\". It probably what made the movie the most interesting to me although it\\'s nowheres near \\\\\"Ran\\'s\\\\\" league. <br /><br />Stylish visual direction and excellent photography keep the movie mostly interesting throughout the two hour and eighteen minute length. The lead actors are uniformly excellent. The music is really good. The two weaknesses are the story and the fight scenes. The movie drags in the middle which could have been fixed by some prudent editing. And the fight scenes are mostly filmed in blurry close-ups. This works for most of the film but the finale feels like a cheat. Another recent film like this is Tsui Hark\\'s \\\\\"Seven Swords\\\\\", great film but the promised fight scenes are disappointing. Asano really doesn\\'t move like a sword wielding demon, his acting is great but he would be an extra in a traditional chambara fight scene.<br /><br />Good movie, you\\'ll probably find it interesting just don\\'t expect traditional sword fights.\"'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Create Logistics Regression model from X and y\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "predictions = lg.predict(X_test)\n",
    "\n",
    "# Show metrics\n",
    "print(\"Accuracy score: %f\" % accuracy_score(y_test, predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Log loss:', log_loss(y_test, predictions)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
